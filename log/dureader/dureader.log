/data/disk1/private/xcj/env/miniconda3/lib/python3.9/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
====================== Initialization ======================
rank :          0
local_rank :    0
world_size :    8
local_size :    8
master :        127.0.0.1:10086
device :        0
cpus :          [0, 1, 2, 3, 4, 5, 6]

====================== Initialization ======================
rank :          1
local_rank :    1
world_size :    8
local_size :    8
master :        127.0.0.1:10086
device :        1
cpus :          [7, 8, 9, 10, 11, 12, 13]

====================== Initialization ======================
rank :          2
local_rank :    2
world_size :    8
local_size :    8
master :        127.0.0.1:10086
device :        2
cpus :          [14, 15, 16, 17, 18, 19, 20]

====================== Initialization ======================
rank :          3
local_rank :    3
world_size :    8
local_size :    8
master :        127.0.0.1:10086
device :        3
cpus :          [21, 22, 23, 24, 25, 26, 27]

====================== Initialization ======================
rank :          4
local_rank :    4
world_size :    8
local_size :    8
master :        127.0.0.1:10086
device :        4
cpus :          [28, 29, 30, 31, 32, 33, 34]

====================== Initialization ======================
rank :          5
local_rank :    5
world_size :    8
local_size :    8
master :        127.0.0.1:10086
device :        5
cpus :          [35, 36, 37, 38, 39, 40, 41]

====================== Initialization ======================
rank :          6
local_rank :    6
world_size :    8
local_size :    8
master :        127.0.0.1:10086
device :        6
cpus :          [42, 43, 44, 45, 46, 47, 48]

====================== Initialization ======================
rank :          7
local_rank :    7
world_size :    8
local_size :    8
master :        127.0.0.1:10086
device :        7
cpus :          [49, 50, 51, 52, 53, 54, 55]

None
02/14/2023 15:07:29 - INFO - __main__ -   CUDA available: True
02/14/2023 15:07:29 - INFO - tools.init_tool -   Begin to initialize dataset and formatter...
read train data: 10799
read valid data: 250
02/14/2023 15:07:30 - INFO - tools.init_tool -   Begin to initialize models...
load from local file: /data/disk1/private/xcj/PLMs/model-center/bert-base-chinese
load from local file: /data/disk1/private/xcj/PLMs/model-center/bert-base-chinese
load from local file: /data/disk1/private/xcj/PLMs/model-center/bert-base-chinese
load from local file: /data/disk1/private/xcj/PLMs/model-center/bert-base-chinese
02/14/2023 15:07:42 - INFO - tools.init_tool -   try load checkpoint from None
02/14/2023 15:07:42 - INFO - tools.init_tool -   Initialize done.
[train]
epoch: 10
batch_size: 32
shuffle: True
reader_num: 4
optimizer: AdamW
learning_rate: 1e-5
weight_decay: 1e-5
warmup_steps: 2000
training_steps: 50000
max_grad_norm: 2.0
scheduler: t5
inspector_para: *
query_len: 32
ctx_len: 512
num_train_passage: 4
========
[eval]
batch_size: 32
reader_num: 4
========
[data]
train_dataset_type: kara
train_formatter_type: DR
train_data_path: ../ChineseQAData/kara/dureader
train_kara_namespace: dureader
train_kara_dataset: train
train_kara_version: lastest
valid_dataset_type: kara
valid_formatter_type: DR
valid_data_path: ../ChineseQAData/kara/dureader
valid_kara_namespace: dureader
valid_kara_dataset: valid
valid_kara_version: lastest
========
[model]
model_name: DR
pretrained_model: bert-base-chinese
========
[output]
output_time: 20
test_time: 1
output_grad_step: 200
model_name: DuReader
output_function: binary
========
None
02/14/2023 15:07:42 - WARNING - tools.train_tool -   Output path exists, check whether need to change a name of model
valid_mode batch no_valid False
step_epoch None
02/14/2023 15:07:42 - INFO - tools.train_tool -   Start training
Epoch  Stage  Iterations  Time Usage    Loss    Output Information
 name                                                                        shape               max       min       std       mean      grad_std  grad_mean
 que_model.input_embedding.weight                                            (21128, 768)        0.6973    -1.0859   0.0543    -0.0016   nan       nan      
 que_model.position_embedding.weight                                         (512, 768)          0.3987    -1.0703   0.0216    0.0000    nan       nan      
 que_model.token_type_embedding.weight                                       (2, 768)            0.2751    -0.6504   0.0303    -0.0001   nan       nan      
 que_model.encoder.layers.0.self_att.layernorm_before_attention.weight       (768,)              0.9883    0.2035    0.1146    0.8867    nan       nan      
 que_model.encoder.layers.0.self_att.layernorm_before_attention.bias         (768,)              2.3809    -0.6758   0.1063    -0.0144   nan       nan      
 que_model.encoder.layers.0.self_att.self_attention.project_q.weight         (768, 768)          0.4685    -0.4412   0.0420    0.0000    nan       nan      
 que_model.encoder.layers.0.self_att.self_attention.project_q.bias           (768,)              1.1602    -1.2285   0.2318    -0.0160   8904.0000 511.2500 
 que_model.encoder.layers.0.self_att.self_attention.project_k.weight         (768, 768)          0.4570    -0.6963   0.0424    -0.0000   nan       nan      
 que_model.encoder.layers.0.self_att.self_attention.project_k.bias           (768,)              0.0143    -0.0173   0.0041    0.0001    0.6284    0.0421   
 que_model.encoder.layers.0.self_att.self_attention.project_v.weight         (768, 768)          0.2615    -0.1899   0.0295    -0.0000   nan       nan      
 que_model.encoder.layers.0.self_att.self_attention.project_v.bias           (768,)              0.4548    -0.3303   0.0592    -0.0009   nan       nan      
 que_model.encoder.layers.0.self_att.self_attention.attention_out.weight     (768, 768)          0.4768    -0.4131   0.0303    0.0000    nan       nan      
 que_model.encoder.layers.0.self_att.self_attention.attention_out.bias       (768,)              0.2456    -0.3198   0.0579    -0.0006   nan       nan      
 que_model.encoder.layers.0.ffn.layernorm_before_ffn.weight                  (768,)              2.8848    0.4226    0.1016    0.8286    nan       nan      
 que_model.encoder.layers.0.ffn.layernorm_before_ffn.bias                    (768,)              1.4629    -4.5469   0.2419    -0.0135   nan       nan      
 que_model.encoder.layers.0.ffn.ffn.w_in.w.weight                            (3072, 768)         0.6099    -0.5239   0.0355    0.0002    nan       nan      
 que_model.encoder.layers.0.ffn.ffn.w_in.w.bias                              (3072,)             0.4456    -0.4915   0.0663    -0.0714   nan       nan      
 que_model.encoder.layers.0.ffn.ffn.w_out.weight                             (768, 3072)         1.8760    -0.9053   0.0359    -0.0000   nan       nan      
 que_model.encoder.layers.0.ffn.ffn.w_out.bias                               (768,)              0.2749    -0.2354   0.0719    0.0002    nan       nan      
 que_model.encoder.layers.1.self_att.layernorm_before_attention.weight       (768,)              0.9819    0.2773    0.0723    0.9062    nan       nan      
 que_model.encoder.layers.1.self_att.layernorm_before_attention.bias         (768,)              0.2371    -1.5693   0.0744    -0.0079   nan       nan      
 que_model.encoder.layers.1.self_att.self_attention.project_q.weight         (768, 768)          0.2588    -0.2489   0.0420    -0.0000   nan       nan      
 que_model.encoder.layers.1.self_att.self_attention.project_q.bias           (768,)              1.1279    -0.7715   0.1808    0.0190    9544.0000 -503.0000
 que_model.encoder.layers.1.self_att.self_attention.project_k.weight         (768, 768)          0.3254    -0.4729   0.0425    -0.0000   nan       nan      
 que_model.encoder.layers.1.self_att.self_attention.project_k.bias           (768,)              0.0315    -0.0300   0.0067    0.0001    1.1035    0.0973   
 que_model.encoder.layers.1.self_att.self_attention.project_v.weight         (768, 768)          0.2358    -0.2098   0.0301    -0.0000   nan       nan      
 que_model.encoder.layers.1.self_att.self_attention.project_v.bias           (768,)              0.2803    -0.2174   0.0393    0.0011    nan       nan      
 que_model.encoder.layers.1.self_att.self_attention.attention_out.weight     (768, 768)          0.2473    -0.4021   0.0292    0.0000    nan       nan      
 que_model.encoder.layers.1.self_att.self_attention.attention_out.bias       (768,)              0.2585    -0.3591   0.0739    -0.0011   nan       nan      
 que_model.encoder.layers.1.ffn.layernorm_before_ffn.weight                  (768,)              2.2207    0.6274    0.0721    0.8794    nan       nan      
 que_model.encoder.layers.1.ffn.layernorm_before_ffn.bias                    (768,)              0.6035    -2.5273   0.1255    -0.0081   nan       nan      
 que_model.encoder.layers.1.ffn.ffn.w_in.w.weight                            (3072, 768)         0.4221    -0.3923   0.0367    0.0000    nan       nan      
 que_model.encoder.layers.1.ffn.ffn.w_in.w.bias                              (3072,)             0.3857    -0.4326   0.0565    -0.0565   nan       nan      
 que_model.encoder.layers.1.ffn.ffn.w_out.weight                             (768, 3072)         2.4766    -1.5996   0.0352    -0.0000   nan       nan      
 que_model.encoder.layers.1.ffn.ffn.w_out.bias                               (768,)              0.2422    -0.2301   0.0726    0.0001    nan       nan      
 que_model.encoder.layers.2.self_att.layernorm_before_attention.weight       (768,)              1.0498    0.3533    0.0562    0.9517    nan       nan      
 que_model.encoder.layers.2.self_att.layernorm_before_attention.bias         (768,)              0.2068    -0.9526   0.0536    -0.0140   nan       nan      
 que_model.encoder.layers.2.self_att.self_attention.project_q.weight         (768, 768)          0.2710    -0.2944   0.0439    0.0000    nan       nan      
 que_model.encoder.layers.2.self_att.self_attention.project_q.bias           (768,)              0.6025    -0.8042   0.1370    0.0027    6252.0000 174.2500 
 que_model.encoder.layers.2.self_att.self_attention.project_k.weight         (768, 768)          0.2832    -0.2966   0.0439    -0.0000   nan       nan      
 que_model.encoder.layers.2.self_att.self_attention.project_k.bias           (768,)              0.0298    -0.0246   0.0064    0.0003    0.5791    0.0110   
 que_model.encoder.layers.2.self_att.self_attention.project_v.weight         (768, 768)          0.1830    -0.2551   0.0308    0.0000    nan       nan      
 que_model.encoder.layers.2.self_att.self_attention.project_v.bias           (768,)              0.1243    -0.1721   0.0306    -0.0005   nan       nan      
 que_model.encoder.layers.2.self_att.self_attention.attention_out.weight     (768, 768)          0.2037    -0.2419   0.0292    -0.0000   nan       nan      
 que_model.encoder.layers.2.self_att.self_attention.attention_out.bias       (768,)              0.3906    -0.2656   0.0853    -0.0004   nan       nan      
 que_model.encoder.layers.2.ffn.layernorm_before_ffn.weight                  (768,)              1.7520    0.6401    0.0602    0.8804    nan       nan      
 que_model.encoder.layers.2.ffn.layernorm_before_ffn.bias                    (768,)              0.4363    -1.5029   0.0866    -0.0070   nan       nan      
 que_model.encoder.layers.2.ffn.ffn.w_in.w.weight                            (3072, 768)         0.5361    -0.6089   0.0380    -0.0001   nan       nan      
 que_model.encoder.layers.2.ffn.ffn.w_in.w.bias                              (3072,)             0.4270    -0.3750   0.0469    -0.0551   nan       nan      
 que_model.encoder.layers.2.ffn.ffn.w_out.weight                             (768, 3072)         3.5918    -1.4189   0.0359    -0.0000   nan       nan      
 que_model.encoder.layers.2.ffn.ffn.w_out.bias                               (768,)              0.2864    -0.3984   0.0793    -0.0001   nan       nan      
 que_model.encoder.layers.3.self_att.layernorm_before_attention.weight       (768,)              1.0098    0.4258    0.0422    0.9189    nan       nan      
 que_model.encoder.layers.3.self_att.layernorm_before_attention.bias         (768,)              0.1292    -0.7759   0.0530    -0.0139   nan       nan      
 que_model.encoder.layers.3.self_att.self_attention.project_q.weight         (768, 768)          0.2432    -0.3130   0.0418    -0.0000   nan       nan      
 que_model.encoder.layers.3.self_att.self_attention.project_q.bias           (768,)              0.6025    -0.4377   0.1157    0.0017    nan       inf      
 que_model.encoder.layers.3.self_att.self_attention.project_k.weight         (768, 768)          0.4331    -0.2605   0.0416    0.0000    nan       nan      
 que_model.encoder.layers.3.self_att.self_attention.project_k.bias           (768,)              0.0217    -0.0168   0.0058    0.0001    0.5303    -0.0157  
 que_model.encoder.layers.3.self_att.self_attention.project_v.weight         (768, 768)          0.3799    -0.3308   0.0324    -0.0000   nan       nan      
 que_model.encoder.layers.3.self_att.self_attention.project_v.bias           (768,)              0.1234    -0.1598   0.0260    -0.0000   nan       nan      
 que_model.encoder.layers.3.self_att.self_attention.attention_out.weight     (768, 768)          0.2479    -0.2859   0.0302    0.0000    nan       nan      
 que_model.encoder.layers.3.self_att.self_attention.attention_out.bias       (768,)              0.1797    -0.1855   0.0640    -0.0002   nan       nan      
 que_model.encoder.layers.3.ffn.layernorm_before_ffn.weight                  (768,)              2.1367    0.6992    0.0768    0.8745    nan       nan      
 que_model.encoder.layers.3.ffn.layernorm_before_ffn.bias                    (768,)              0.5244    -1.0957   0.0824    -0.0085   nan       nan      
 que_model.encoder.layers.3.ffn.ffn.w_in.w.weight                            (3072, 768)         0.6470    -0.6426   0.0378    -0.0001   nan       nan      
 que_model.encoder.layers.3.ffn.ffn.w_in.w.bias                              (3072,)             0.3015    -0.3467   0.0436    -0.0523   nan       nan      
 que_model.encoder.layers.3.ffn.ffn.w_out.weight                             (768, 3072)         5.3906    -1.6758   0.0358    -0.0000   nan       nan      
 que_model.encoder.layers.3.ffn.ffn.w_out.bias                               (768,)              0.3110    -0.4966   0.0714    -0.0002   nan       nan      
 que_model.encoder.layers.4.self_att.layernorm_before_attention.weight       (768,)              1.0469    0.3303    0.0446    0.9492    nan       nan      
 que_model.encoder.layers.4.self_att.layernorm_before_attention.bias         (768,)              0.0989    -0.7705   0.0438    -0.0116   nan       nan      
 que_model.encoder.layers.4.self_att.self_attention.project_q.weight         (768, 768)          0.3135    -0.2644   0.0433    0.0000    nan       nan      
 que_model.encoder.layers.4.self_att.self_attention.project_q.bias           (768,)              0.5210    -0.5967   0.1176    0.0013    nan       nan      
 que_model.encoder.layers.4.self_att.self_attention.project_k.weight         (768, 768)          0.3389    -0.3281   0.0429    -0.0001   nan       nan      
 que_model.encoder.layers.4.self_att.self_attention.project_k.bias           (768,)              0.0216    -0.0226   0.0057    0.0001    0.4397    0.0154   
 que_model.encoder.layers.4.self_att.self_attention.project_v.weight         (768, 768)          0.2058    -0.2186   0.0364    0.0000    nan       nan      
 que_model.encoder.layers.4.self_att.self_attention.project_v.bias           (768,)              0.0854    -0.1187   0.0181    0.0003    nan       nan      
 que_model.encoder.layers.4.self_att.self_attention.attention_out.weight     (768, 768)          0.3306    -0.4290   0.0335    0.0000    nan       nan      
 que_model.encoder.layers.4.self_att.self_attention.attention_out.bias       (768,)              0.2791    -0.2310   0.0621    -0.0004   nan       nan      
 que_model.encoder.layers.4.ffn.layernorm_before_ffn.weight                  (768,)              2.6055    0.6973    0.0941    0.8594    nan       nan      
 que_model.encoder.layers.4.ffn.layernorm_before_ffn.bias                    (768,)              0.3711    -1.0098   0.0754    -0.0134   nan       nan      
 que_model.encoder.layers.4.ffn.ffn.w_in.w.weight                            (3072, 768)         0.7031    -0.6372   0.0382    -0.0001   nan       nan      
 que_model.encoder.layers.4.ffn.ffn.w_in.w.bias                              (3072,)             0.3206    -0.3997   0.0494    -0.0487   nan       nan      
 que_model.encoder.layers.4.ffn.ffn.w_out.weight                             (768, 3072)         5.2734    -0.9897   0.0356    -0.0000   nan       nan      
 que_model.encoder.layers.4.ffn.ffn.w_out.bias                               (768,)              0.3213    -0.5020   0.0746    -0.0002   nan       nan      
 que_model.encoder.layers.5.self_att.layernorm_before_attention.weight       (768,)              1.1729    0.3757    0.0467    0.9897    nan       nan      
 que_model.encoder.layers.5.self_att.layernorm_before_attention.bias         (768,)              0.0923    -0.6836   0.0428    -0.0114   nan       nan      
 que_model.encoder.layers.5.self_att.self_attention.project_q.weight         (768, 768)          0.2842    -0.2534   0.0458    -0.0000   nan       nan      
 que_model.encoder.layers.5.self_att.self_attention.project_q.bias           (768,)              0.5815    -0.4766   0.1104    0.0038    nan       nan      
 que_model.encoder.layers.5.self_att.self_attention.project_k.weight         (768, 768)          0.3154    -0.2881   0.0450    -0.0000   nan       nan      
 que_model.encoder.layers.5.self_att.self_attention.project_k.bias           (768,)              0.0273    -0.0271   0.0066    0.0002    0.4873    0.0288   
 que_model.encoder.layers.5.self_att.self_attention.project_v.weight         (768, 768)          0.2028    -0.2120   0.0359    -0.0000   nan       nan      
 que_model.encoder.layers.5.self_att.self_attention.project_v.bias           (768,)              0.1068    -0.1387   0.0214    0.0006    nan       nan      
 que_model.encoder.layers.5.self_att.self_attention.attention_out.weight     (768, 768)          0.2820    -0.2075   0.0324    0.0000    nan       nan      
 que_model.encoder.layers.5.self_att.self_attention.attention_out.bias       (768,)              0.5625    -0.2228   0.0493    0.0002    nan       nan      
 que_model.encoder.layers.5.ffn.layernorm_before_ffn.weight                  (768,)              2.5430    0.6982    0.1042    0.8540    nan       nan      
 que_model.encoder.layers.5.ffn.layernorm_before_ffn.bias                    (768,)              0.4653    -1.0654   0.0833    -0.0141   nan       nan      
 que_model.encoder.layers.5.ffn.ffn.w_in.w.weight                            (3072, 768)         0.5034    -0.5200   0.0382    -0.0002   nan       nan      
 que_model.encoder.layers.5.ffn.ffn.w_in.w.bias                              (3072,)             0.3655    -0.3733   0.0569    -0.0474   nan       nan      
 que_model.encoder.layers.5.ffn.ffn.w_out.weight                             (768, 3072)         6.2539    -1.2070   0.0358    -0.0000   nan       nan      
 que_model.encoder.layers.5.ffn.ffn.w_out.bias                               (768,)              0.2413    -0.4766   0.0775    -0.0002   nan       nan      
 que_model.encoder.layers.6.self_att.layernorm_before_attention.weight       (768,)              1.1465    0.4768    0.0435    0.9946    nan       nan      
 que_model.encoder.layers.6.self_att.layernorm_before_attention.bias         (768,)              0.0961    -0.9268   0.0500    -0.0055   nan       nan      
 que_model.encoder.layers.6.self_att.self_attention.project_q.weight         (768, 768)          0.4255    -0.3201   0.0486    0.0000    nan       nan      
 que_model.encoder.layers.6.self_att.self_attention.project_q.bias           (768,)              0.4880    -0.4524   0.1176    0.0070    7508.0000 537.5000 
 que_model.encoder.layers.6.self_att.self_attention.project_k.weight         (768, 768)          0.3130    -0.3352   0.0466    -0.0000   nan       nan      
 que_model.encoder.layers.6.self_att.self_attention.project_k.bias           (768,)              0.0294    -0.0199   0.0057    0.0004    0.4143    0.0093   
 que_model.encoder.layers.6.self_att.self_attention.project_v.weight         (768, 768)          0.2091    -0.2068   0.0355    0.0001    nan       nan      
 que_model.encoder.layers.6.self_att.self_attention.project_v.bias           (768,)              0.0846    -0.1664   0.0257    -0.0005   nan       nan      
 que_model.encoder.layers.6.self_att.self_attention.attention_out.weight     (768, 768)          0.2349    -0.2681   0.0325    0.0000    nan       nan      
 que_model.encoder.layers.6.self_att.self_attention.attention_out.bias       (768,)              0.8989    -0.2722   0.0607    0.0005    nan       nan      
 que_model.encoder.layers.6.ffn.layernorm_before_ffn.weight                  (768,)              2.5527    0.6948    0.0978    0.8638    nan       nan      
 que_model.encoder.layers.6.ffn.layernorm_before_ffn.bias                    (768,)              0.6436    -1.8086   0.0945    -0.0159   nan       nan      
 que_model.encoder.layers.6.ffn.ffn.w_in.w.weight                            (3072, 768)         0.7007    -0.7964   0.0384    -0.0002   nan       nan      
 que_model.encoder.layers.6.ffn.ffn.w_in.w.bias                              (3072,)             0.3269    -0.4194   0.0608    -0.0490   nan       nan      
 que_model.encoder.layers.6.ffn.ffn.w_out.weight                             (768, 3072)         7.2539    -0.7451   0.0363    -0.0000   nan       nan      
 que_model.encoder.layers.6.ffn.ffn.w_out.bias                               (768,)              0.3132    -0.6548   0.0774    -0.0001   nan       nan      
 que_model.encoder.layers.7.self_att.layernorm_before_attention.weight       (768,)              1.0752    0.4927    0.0353    0.9639    nan       nan      
 que_model.encoder.layers.7.self_att.layernorm_before_attention.bias         (768,)              0.1799    -1.0996   0.0527    -0.0028   nan       nan      
 que_model.encoder.layers.7.self_att.self_attention.project_q.weight         (768, 768)          0.2644    -0.2632   0.0462    -0.0001   nan       nan      
 que_model.encoder.layers.7.self_att.self_attention.project_q.bias           (768,)              0.5835    -0.5942   0.1384    -0.0018   7608.0000 -265.5000
 que_model.encoder.layers.7.self_att.self_attention.project_k.weight         (768, 768)          0.2976    -0.2622   0.0449    0.0001    nan       nan      
 que_model.encoder.layers.7.self_att.self_attention.project_k.bias           (768,)              0.0258    -0.0220   0.0059    0.0004    0.3809    -0.0206  
 que_model.encoder.layers.7.self_att.self_attention.project_v.weight         (768, 768)          0.1820    -0.1847   0.0364    -0.0000   nan       nan      
 que_model.encoder.layers.7.self_att.self_attention.project_v.bias           (768,)              0.1813    -0.1494   0.0339    -0.0014   nan       nan      
 que_model.encoder.layers.7.self_att.self_attention.attention_out.weight     (768, 768)          0.2303    -0.2485   0.0337    0.0000    nan       nan      
 que_model.encoder.layers.7.self_att.self_attention.attention_out.bias       (768,)              0.8164    -0.4888   0.0619    0.0005    nan       nan      
 que_model.encoder.layers.7.ffn.layernorm_before_ffn.weight                  (768,)              2.4375    0.7051    0.0845    0.8335    nan       nan      
 que_model.encoder.layers.7.ffn.layernorm_before_ffn.bias                    (768,)              0.7212    -1.8779   0.0909    -0.0147   nan       nan      
 que_model.encoder.layers.7.ffn.ffn.w_in.w.weight                            (3072, 768)         0.2917    -0.3008   0.0370    -0.0002   nan       nan      
 que_model.encoder.layers.7.ffn.ffn.w_in.w.bias                              (3072,)             0.2864    -0.3928   0.0560    -0.0470   nan       nan      
 que_model.encoder.layers.7.ffn.ffn.w_out.weight                             (768, 3072)         6.0234    -0.5698   0.0354    -0.0000   nan       nan      
 que_model.encoder.layers.7.ffn.ffn.w_out.bias                               (768,)              0.4167    -0.6602   0.0853    0.0002    nan       nan      
 que_model.encoder.layers.8.self_att.layernorm_before_attention.weight       (768,)              1.2637    0.5249    0.0299    0.9590    nan       nan      
 que_model.encoder.layers.8.self_att.layernorm_before_attention.bias         (768,)              0.4993    -0.8975   0.0443    -0.0010   nan       nan      
 que_model.encoder.layers.8.self_att.self_attention.project_q.weight         (768, 768)          0.2544    -0.2576   0.0449    0.0000    nan       nan      
 que_model.encoder.layers.8.self_att.self_attention.project_q.bias           (768,)              0.5479    -0.6108   0.1389    0.0007    11040.000 -531.5000
 que_model.encoder.layers.8.self_att.self_attention.project_k.weight         (768, 768)          0.3157    -0.2759   0.0439    -0.0000   nan       nan      
 que_model.encoder.layers.8.self_att.self_attention.project_k.bias           (768,)              0.0184    -0.0219   0.0057    -0.0001   0.5239    -0.0167  
 que_model.encoder.layers.8.self_att.self_attention.project_v.weight         (768, 768)          0.1986    -0.1844   0.0376    0.0000    nan       nan      
 que_model.encoder.layers.8.self_att.self_attention.project_v.bias           (768,)              0.2091    -0.1760   0.0326    0.0010    nan       nan      
 que_model.encoder.layers.8.self_att.self_attention.attention_out.weight     (768, 768)          0.1908    -0.2037   0.0347    -0.0000   nan       nan      
 que_model.encoder.layers.8.self_att.self_attention.attention_out.bias       (768,)              0.4666    -0.6069   0.0610    0.0001    nan       nan      
 que_model.encoder.layers.8.ffn.layernorm_before_ffn.weight                  (768,)              1.8838    0.6914    0.0597    0.8096    nan       nan      
 que_model.encoder.layers.8.ffn.layernorm_before_ffn.bias                    (768,)              0.5806    -1.3232   0.0762    -0.0139   nan       nan      
 que_model.encoder.layers.8.ffn.ffn.w_in.w.weight                            (3072, 768)         0.4382    -0.2910   0.0368    -0.0001   nan       nan      
 que_model.encoder.layers.8.ffn.ffn.w_in.w.bias                              (3072,)             0.3972    -0.3625   0.0493    -0.0481   nan       nan      
 que_model.encoder.layers.8.ffn.ffn.w_out.weight                             (768, 3072)         5.2500    -1.0166   0.0361    -0.0000   nan       nan      
 que_model.encoder.layers.8.ffn.ffn.w_out.bias                               (768,)              0.3899    -0.5708   0.0799    -0.0000   nan       nan      
 que_model.encoder.layers.9.self_att.layernorm_before_attention.weight       (768,)              1.7256    0.6650    0.0357    0.9644    nan       nan      
 que_model.encoder.layers.9.self_att.layernorm_before_attention.bias         (768,)              0.7881    -0.5981   0.0439    0.0015    nan       nan      
 que_model.encoder.layers.9.self_att.self_attention.project_q.weight         (768, 768)          0.2224    -0.4380   0.0443    -0.0000   nan       nan      
 que_model.encoder.layers.9.self_att.self_attention.project_q.bias           (768,)              0.3979    -0.4722   0.1403    -0.0082   nan       inf      
 que_model.encoder.layers.9.self_att.self_attention.project_k.weight         (768, 768)          0.3433    -0.2673   0.0436    0.0000    nan       nan      
 que_model.encoder.layers.9.self_att.self_attention.project_k.bias           (768,)              0.0266    -0.0243   0.0061    0.0004    0.5674    0.0278   
 que_model.encoder.layers.9.self_att.self_attention.project_v.weight         (768, 768)          0.2014    -0.1780   0.0357    0.0000    nan       nan      
 que_model.encoder.layers.9.self_att.self_attention.project_v.bias           (768,)              0.1313    -0.1469   0.0265    0.0000    nan       nan      
 que_model.encoder.layers.9.self_att.self_attention.attention_out.weight     (768, 768)          0.3108    -0.2996   0.0322    0.0000    nan       nan      
 que_model.encoder.layers.9.self_att.self_attention.attention_out.bias       (768,)              0.5044    -0.5752   0.0628    0.0003    nan       nan      
 que_model.encoder.layers.9.ffn.layernorm_before_ffn.weight                  (768,)              1.7803    0.6450    0.0514    0.8140    nan       nan      
 que_model.encoder.layers.9.ffn.layernorm_before_ffn.bias                    (768,)              0.5898    -0.7627   0.0620    -0.0025   nan       nan      
 que_model.encoder.layers.9.ffn.ffn.w_in.w.weight                            (3072, 768)         0.3401    -0.2959   0.0370    -0.0002   nan       nan      
 que_model.encoder.layers.9.ffn.ffn.w_in.w.bias                              (3072,)             0.2067    -0.3523   0.0417    -0.0483   nan       nan      
 que_model.encoder.layers.9.ffn.ffn.w_out.weight                             (768, 3072)         4.4805    -0.7744   0.0369    -0.0000   nan       nan      
 que_model.encoder.layers.9.ffn.ffn.w_out.bias                               (768,)              0.5112    -0.4136   0.0726    -0.0001   nan       nan      
 que_model.encoder.layers.10.self_att.layernorm_before_attention.weight      (768,)              1.9053    0.6577    0.0408    0.9648    nan       nan      
 que_model.encoder.layers.10.self_att.layernorm_before_attention.bias        (768,)              0.9111    -0.3618   0.0492    0.0114    nan       nan      
 que_model.encoder.layers.10.self_att.self_attention.project_q.weight        (768, 768)          0.2146    -0.2367   0.0450    0.0001    nan       nan      
 que_model.encoder.layers.10.self_att.self_attention.project_q.bias          (768,)              0.7012    -0.7715   0.1681    0.0074    8776.0000 322.0000 
 que_model.encoder.layers.10.self_att.self_attention.project_k.weight        (768, 768)          0.2593    -0.2734   0.0443    -0.0000   nan       nan      
 que_model.encoder.layers.10.self_att.self_attention.project_k.bias          (768,)              0.0275    -0.0334   0.0066    0.0000    0.5674    -0.0053  
 que_model.encoder.layers.10.self_att.self_attention.project_v.weight        (768, 768)          0.1831    -0.1777   0.0353    0.0000    nan       nan      
 que_model.encoder.layers.10.self_att.self_attention.project_v.bias          (768,)              0.2202    -0.1782   0.0359    -0.0003   nan       nan      
 que_model.encoder.layers.10.self_att.self_attention.attention_out.weight    (768, 768)          0.3616    -0.3479   0.0319    -0.0000   nan       nan      
 que_model.encoder.layers.10.self_att.self_attention.attention_out.bias      (768,)              0.3875    -0.6060   0.0677    0.0002    nan       nan      
 que_model.encoder.layers.10.ffn.layernorm_before_ffn.weight                 (768,)              2.8047    0.6372    0.0759    0.8706    nan       nan      
 que_model.encoder.layers.10.ffn.layernorm_before_ffn.bias                   (768,)              0.6445    -0.6567   0.0656    0.0069    nan       nan      
 que_model.encoder.layers.10.ffn.ffn.w_in.w.weight                           (3072, 768)         0.3535    -0.2717   0.0380    -0.0001   nan       nan      
 que_model.encoder.layers.10.ffn.ffn.w_in.w.bias                             (3072,)             0.3538    -0.2477   0.0302    -0.0476   nan       nan      
 que_model.encoder.layers.10.ffn.ffn.w_out.weight                            (768, 3072)         11.5781   -1.8086   0.0375    0.0000    nan       nan      
 que_model.encoder.layers.10.ffn.ffn.w_out.bias                              (768,)              0.5063    -0.2345   0.0596    0.0001    nan       nan      
 que_model.encoder.layers.11.self_att.layernorm_before_attention.weight      (768,)              1.6553    0.5049    0.0347    0.9663    nan       nan      
 que_model.encoder.layers.11.self_att.layernorm_before_attention.bias        (768,)              0.8457    -1.5586   0.0765    0.0176    nan       nan      
 que_model.encoder.layers.11.self_att.self_attention.project_q.weight        (768, 768)          0.2126    -0.2150   0.0448    -0.0001   nan       nan      
 que_model.encoder.layers.11.self_att.self_attention.project_q.bias          (768,)              0.5225    -0.5620   0.1833    -0.0138   7388.0000 -207.1250
 que_model.encoder.layers.11.self_att.self_attention.project_k.weight        (768, 768)          0.2703    -0.2515   0.0436    0.0001    nan       nan      
 que_model.encoder.layers.11.self_att.self_attention.project_k.bias          (768,)              0.0203    -0.0233   0.0062    -0.0000   0.3887    -0.0115  
 que_model.encoder.layers.11.self_att.self_attention.project_v.weight        (768, 768)          0.2479    -0.2107   0.0403    -0.0001   nan       nan      
 que_model.encoder.layers.11.self_att.self_attention.project_v.bias          (768,)              0.1011    -0.0900   0.0235    -0.0001   nan       nan      
 que_model.encoder.layers.11.self_att.self_attention.attention_out.weight    (768, 768)          1.1289    -1.1084   0.0375    0.0000    nan       nan      
 que_model.encoder.layers.11.self_att.self_attention.attention_out.bias      (768,)              0.2664    -0.4788   0.0490    -0.0000   nan       nan      
 que_model.encoder.layers.11.ffn.layernorm_before_ffn.weight                 (768,)              1.0146    0.1737    0.0338    0.8687    nan       nan      
 que_model.encoder.layers.11.ffn.layernorm_before_ffn.bias                   (768,)              1.1953    -0.1689   0.0853    0.0134    nan       nan      
 que_model.encoder.layers.11.ffn.ffn.w_in.w.weight                           (3072, 768)         0.2666    -0.2373   0.0378    0.0002    nan       nan      
 que_model.encoder.layers.11.ffn.ffn.w_in.w.bias                             (3072,)             0.8213    -0.4004   0.0554    -0.0498   nan       nan      
 que_model.encoder.layers.11.ffn.ffn.w_out.weight                            (768, 3072)         2.1152    -0.9043   0.0357    -0.0000   nan       nan      
 que_model.encoder.layers.11.ffn.ffn.w_out.bias                              (768,)              0.3083    -0.1350   0.0458    0.0003    nan       nan      
 que_model.encoder.output_layernorm.weight                                   (768,)              1.1426    0.5776    0.0552    0.8110    nan       nan      
 que_model.encoder.output_layernorm.bias                                     (768,)              1.6982    -0.5469   0.1503    0.0000    nan       nan      
 que_model.lm_head.dense.weight                                              (768, 768)          1.1729    -1.1133   0.0511    -0.0011   0.0000    0.0000   
 que_model.lm_head.dense.bias                                                (768,)              4.2969    -0.7007   0.2524    0.0617    0.0000    0.0000   
 que_model.lm_head.layer_norm.weight                                         (768,)              3.4258    0.1548    0.3772    2.5840    0.0000    0.0000   
 que_model.lm_head.layer_norm.bias                                           (768,)              1.6289    -2.0879   0.3225    0.1052    0.0000    0.0000   
 que_model.lm_head.decoder.weight                                            (21128, 768)        0.6973    -1.0859   0.0543    -0.0016   0.0000    0.0000   
 que_model.lm_head.decoder.bias                                              (21128,)            2.3516    -5.0078   0.1404    -0.2234   0.0000    0.0000   
 que_model.pooler.dense.weight                                               (768, 768)          0.2971    -0.2421   0.0421    0.0000    0.0000    0.0000   
 que_model.pooler.dense.bias                                                 (768,)              0.0852    -0.0817   0.0423    0.0026    0.0000    0.0000   
 ctx_model.input_embedding.weight                                            (21128, 768)        0.6973    -1.0859   0.0543    -0.0016   nan       nan      
 ctx_model.position_embedding.weight                                         (512, 768)          0.3987    -1.0703   0.0216    0.0000    nan       nan      
 ctx_model.token_type_embedding.weight                                       (2, 768)            0.2751    -0.6504   0.0303    -0.0001   nan       nan      
 ctx_model.encoder.layers.0.self_att.layernorm_before_attention.weight       (768,)              0.9883    0.2035    0.1146    0.8867    nan       nan      
 ctx_model.encoder.layers.0.self_att.layernorm_before_attention.bias         (768,)              2.3809    -0.6758   0.1063    -0.0144   nan       nan      
 ctx_model.encoder.layers.0.self_att.self_attention.project_q.weight         (768, 768)          0.4685    -0.4412   0.0420    0.0000    nan       nan      
 ctx_model.encoder.layers.0.self_att.self_attention.project_q.bias           (768,)              1.1602    -1.2285   0.2318    -0.0160   nan       nan      
 ctx_model.encoder.layers.0.self_att.self_attention.project_k.weight         (768, 768)          0.4570    -0.6963   0.0424    -0.0000   nan       nan      
 ctx_model.encoder.layers.0.self_att.self_attention.project_k.bias           (768,)              0.0143    -0.0173   0.0041    0.0001    nan       nan      
 ctx_model.encoder.layers.0.self_att.self_attention.project_v.weight         (768, 768)          0.2615    -0.1899   0.0295    -0.0000   nan       nan      
 ctx_model.encoder.layers.0.self_att.self_attention.project_v.bias           (768,)              0.4548    -0.3303   0.0592    -0.0009   nan       nan      
 ctx_model.encoder.layers.0.self_att.self_attention.attention_out.weight     (768, 768)          0.4768    -0.4131   0.0303    0.0000    nan       nan      
 ctx_model.encoder.layers.0.self_att.self_attention.attention_out.bias       (768,)              0.2456    -0.3198   0.0579    -0.0006   nan       nan      
 ctx_model.encoder.layers.0.ffn.layernorm_before_ffn.weight                  (768,)              2.8848    0.4226    0.1016    0.8286    nan       nan      
 ctx_model.encoder.layers.0.ffn.layernorm_before_ffn.bias                    (768,)              1.4629    -4.5469   0.2419    -0.0135   nan       nan      
 ctx_model.encoder.layers.0.ffn.ffn.w_in.w.weight                            (3072, 768)         0.6099    -0.5239   0.0355    0.0002    nan       nan      
 ctx_model.encoder.layers.0.ffn.ffn.w_in.w.bias                              (3072,)             0.4456    -0.4915   0.0663    -0.0714   nan       nan      
 ctx_model.encoder.layers.0.ffn.ffn.w_out.weight                             (768, 3072)         1.8760    -0.9053   0.0359    -0.0000   nan       nan      
 ctx_model.encoder.layers.0.ffn.ffn.w_out.bias                               (768,)              0.2749    -0.2354   0.0719    0.0002    nan       nan      
 ctx_model.encoder.layers.1.self_att.layernorm_before_attention.weight       (768,)              0.9819    0.2773    0.0723    0.9062    nan       nan      
 ctx_model.encoder.layers.1.self_att.layernorm_before_attention.bias         (768,)              0.2371    -1.5693   0.0744    -0.0079   nan       nan      
 ctx_model.encoder.layers.1.self_att.self_attention.project_q.weight         (768, 768)          0.2588    -0.2489   0.0420    -0.0000   nan       nan      
 ctx_model.encoder.layers.1.self_att.self_attention.project_q.bias           (768,)              1.1279    -0.7715   0.1808    0.0190    nan       nan      
 ctx_model.encoder.layers.1.self_att.self_attention.project_k.weight         (768, 768)          0.3254    -0.4729   0.0425    -0.0000   nan       nan      
 ctx_model.encoder.layers.1.self_att.self_attention.project_k.bias           (768,)              0.0315    -0.0300   0.0067    0.0001    nan       nan      
 ctx_model.encoder.layers.1.self_att.self_attention.project_v.weight         (768, 768)          0.2358    -0.2098   0.0301    -0.0000   nan       nan      
 ctx_model.encoder.layers.1.self_att.self_attention.project_v.bias           (768,)              0.2803    -0.2174   0.0393    0.0011    nan       nan      
 ctx_model.encoder.layers.1.self_att.self_attention.attention_out.weight     (768, 768)          0.2473    -0.4021   0.0292    0.0000    nan       nan      
 ctx_model.encoder.layers.1.self_att.self_attention.attention_out.bias       (768,)              0.2585    -0.3591   0.0739    -0.0011   nan       nan      
 ctx_model.encoder.layers.1.ffn.layernorm_before_ffn.weight                  (768,)              2.2207    0.6274    0.0721    0.8794    nan       nan      
 ctx_model.encoder.layers.1.ffn.layernorm_before_ffn.bias                    (768,)              0.6035    -2.5273   0.1255    -0.0081   nan       nan      
 ctx_model.encoder.layers.1.ffn.ffn.w_in.w.weight                            (3072, 768)         0.4221    -0.3923   0.0367    0.0000    nan       nan      
 ctx_model.encoder.layers.1.ffn.ffn.w_in.w.bias                              (3072,)             0.3857    -0.4326   0.0565    -0.0565   nan       nan      
 ctx_model.encoder.layers.1.ffn.ffn.w_out.weight                             (768, 3072)         2.4766    -1.5996   0.0352    -0.0000   nan       nan      
 ctx_model.encoder.layers.1.ffn.ffn.w_out.bias                               (768,)              0.2422    -0.2301   0.0726    0.0001    nan       nan      
 ctx_model.encoder.layers.2.self_att.layernorm_before_attention.weight       (768,)              1.0498    0.3533    0.0562    0.9517    nan       nan      
 ctx_model.encoder.layers.2.self_att.layernorm_before_attention.bias         (768,)              0.2068    -0.9526   0.0536    -0.0140   nan       nan      
 ctx_model.encoder.layers.2.self_att.self_attention.project_q.weight         (768, 768)          0.2710    -0.2944   0.0439    0.0000    nan       nan      
 ctx_model.encoder.layers.2.self_att.self_attention.project_q.bias           (768,)              0.6025    -0.8042   0.1370    0.0027    nan       nan      
 ctx_model.encoder.layers.2.self_att.self_attention.project_k.weight         (768, 768)          0.2832    -0.2966   0.0439    -0.0000   nan       nan      
 ctx_model.encoder.layers.2.self_att.self_attention.project_k.bias           (768,)              0.0298    -0.0246   0.0064    0.0003    nan       nan      
 ctx_model.encoder.layers.2.self_att.self_attention.project_v.weight         (768, 768)          0.1830    -0.2551   0.0308    0.0000    nan       nan      
 ctx_model.encoder.layers.2.self_att.self_attention.project_v.bias           (768,)              0.1243    -0.1721   0.0306    -0.0005   nan       nan      
 ctx_model.encoder.layers.2.self_att.self_attention.attention_out.weight     (768, 768)          0.2037    -0.2419   0.0292    -0.0000   nan       nan      
 ctx_model.encoder.layers.2.self_att.self_attention.attention_out.bias       (768,)              0.3906    -0.2656   0.0853    -0.0004   nan       nan      
 ctx_model.encoder.layers.2.ffn.layernorm_before_ffn.weight                  (768,)              1.7520    0.6401    0.0602    0.8804    nan       nan      
 ctx_model.encoder.layers.2.ffn.layernorm_before_ffn.bias                    (768,)              0.4363    -1.5029   0.0866    -0.0070   nan       nan      
 ctx_model.encoder.layers.2.ffn.ffn.w_in.w.weight                            (3072, 768)         0.5361    -0.6089   0.0380    -0.0001   nan       nan      
 ctx_model.encoder.layers.2.ffn.ffn.w_in.w.bias                              (3072,)             0.4270    -0.3750   0.0469    -0.0551   nan       nan      
 ctx_model.encoder.layers.2.ffn.ffn.w_out.weight                             (768, 3072)         3.5918    -1.4189   0.0359    -0.0000   nan       nan      
 ctx_model.encoder.layers.2.ffn.ffn.w_out.bias                               (768,)              0.2864    -0.3984   0.0793    -0.0001   nan       nan      
 ctx_model.encoder.layers.3.self_att.layernorm_before_attention.weight       (768,)              1.0098    0.4258    0.0422    0.9189    nan       nan      
 ctx_model.encoder.layers.3.self_att.layernorm_before_attention.bias         (768,)              0.1292    -0.7759   0.0530    -0.0139   nan       nan      
 ctx_model.encoder.layers.3.self_att.self_attention.project_q.weight         (768, 768)          0.2432    -0.3130   0.0418    -0.0000   nan       nan      
 ctx_model.encoder.layers.3.self_att.self_attention.project_q.bias           (768,)              0.6025    -0.4377   0.1157    0.0017    nan       nan      
 ctx_model.encoder.layers.3.self_att.self_attention.project_k.weight         (768, 768)          0.4331    -0.2605   0.0416    0.0000    nan       nan      
 ctx_model.encoder.layers.3.self_att.self_attention.project_k.bias           (768,)              0.0217    -0.0168   0.0058    0.0001    nan       nan      
 ctx_model.encoder.layers.3.self_att.self_attention.project_v.weight         (768, 768)          0.3799    -0.3308   0.0324    -0.0000   nan       nan      
 ctx_model.encoder.layers.3.self_att.self_attention.project_v.bias           (768,)              0.1234    -0.1598   0.0260    -0.0000   nan       nan      
 ctx_model.encoder.layers.3.self_att.self_attention.attention_out.weight     (768, 768)          0.2479    -0.2859   0.0302    0.0000    nan       nan      
 ctx_model.encoder.layers.3.self_att.self_attention.attention_out.bias       (768,)              0.1797    -0.1855   0.0640    -0.0002   nan       nan      
 ctx_model.encoder.layers.3.ffn.layernorm_before_ffn.weight                  (768,)              2.1367    0.6992    0.0768    0.8745    nan       nan      
 ctx_model.encoder.layers.3.ffn.layernorm_before_ffn.bias                    (768,)              0.5244    -1.0957   0.0824    -0.0085   nan       nan      
 ctx_model.encoder.layers.3.ffn.ffn.w_in.w.weight                            (3072, 768)         0.6470    -0.6426   0.0378    -0.0001   nan       nan      
 ctx_model.encoder.layers.3.ffn.ffn.w_in.w.bias                              (3072,)             0.3015    -0.3467   0.0436    -0.0523   nan       nan      
 ctx_model.encoder.layers.3.ffn.ffn.w_out.weight                             (768, 3072)         5.3906    -1.6758   0.0358    -0.0000   nan       nan      
 ctx_model.encoder.layers.3.ffn.ffn.w_out.bias                               (768,)              0.3110    -0.4966   0.0714    -0.0002   nan       nan      
 ctx_model.encoder.layers.4.self_att.layernorm_before_attention.weight       (768,)              1.0469    0.3303    0.0446    0.9492    nan       nan      
 ctx_model.encoder.layers.4.self_att.layernorm_before_attention.bias         (768,)              0.0989    -0.7705   0.0438    -0.0116   nan       nan      
 ctx_model.encoder.layers.4.self_att.self_attention.project_q.weight         (768, 768)          0.3135    -0.2644   0.0433    0.0000    nan       nan      
 ctx_model.encoder.layers.4.self_att.self_attention.project_q.bias           (768,)              0.5210    -0.5967   0.1176    0.0013    nan       nan      
 ctx_model.encoder.layers.4.self_att.self_attention.project_k.weight         (768, 768)          0.3389    -0.3281   0.0429    -0.0001   nan       nan      
 ctx_model.encoder.layers.4.self_att.self_attention.project_k.bias           (768,)              0.0216    -0.0226   0.0057    0.0001    nan       nan      
 ctx_model.encoder.layers.4.self_att.self_attention.project_v.weight         (768, 768)          0.2058    -0.2186   0.0364    0.0000    nan       nan      
 ctx_model.encoder.layers.4.self_att.self_attention.project_v.bias           (768,)              0.0854    -0.1187   0.0181    0.0003    nan       nan      
 ctx_model.encoder.layers.4.self_att.self_attention.attention_out.weight     (768, 768)          0.3306    -0.4290   0.0335    0.0000    nan       nan      
 ctx_model.encoder.layers.4.self_att.self_attention.attention_out.bias       (768,)              0.2791    -0.2310   0.0621    -0.0004   nan       nan      
 ctx_model.encoder.layers.4.ffn.layernorm_before_ffn.weight                  (768,)              2.6055    0.6973    0.0941    0.8594    nan       nan      
 ctx_model.encoder.layers.4.ffn.layernorm_before_ffn.bias                    (768,)              0.3711    -1.0098   0.0754    -0.0134   nan       nan      
 ctx_model.encoder.layers.4.ffn.ffn.w_in.w.weight                            (3072, 768)         0.7031    -0.6372   0.0382    -0.0001   nan       nan      
 ctx_model.encoder.layers.4.ffn.ffn.w_in.w.bias                              (3072,)             0.3206    -0.3997   0.0494    -0.0487   nan       nan      
 ctx_model.encoder.layers.4.ffn.ffn.w_out.weight                             (768, 3072)         5.2734    -0.9897   0.0356    -0.0000   nan       nan      
 ctx_model.encoder.layers.4.ffn.ffn.w_out.bias                               (768,)              0.3213    -0.5020   0.0746    -0.0002   nan       nan      
 ctx_model.encoder.layers.5.self_att.layernorm_before_attention.weight       (768,)              1.1729    0.3757    0.0467    0.9897    nan       nan      
 ctx_model.encoder.layers.5.self_att.layernorm_before_attention.bias         (768,)              0.0923    -0.6836   0.0428    -0.0114   nan       nan      
 ctx_model.encoder.layers.5.self_att.self_attention.project_q.weight         (768, 768)          0.2842    -0.2534   0.0458    -0.0000   nan       nan      
 ctx_model.encoder.layers.5.self_att.self_attention.project_q.bias           (768,)              0.5815    -0.4766   0.1104    0.0038    nan       nan      
 ctx_model.encoder.layers.5.self_att.self_attention.project_k.weight         (768, 768)          0.3154    -0.2881   0.0450    -0.0000   nan       nan      
 ctx_model.encoder.layers.5.self_att.self_attention.project_k.bias           (768,)              0.0273    -0.0271   0.0066    0.0002    nan       nan      
 ctx_model.encoder.layers.5.self_att.self_attention.project_v.weight         (768, 768)          0.2028    -0.2120   0.0359    -0.0000   nan       nan      
 ctx_model.encoder.layers.5.self_att.self_attention.project_v.bias           (768,)              0.1068    -0.1387   0.0214    0.0006    nan       nan      
 ctx_model.encoder.layers.5.self_att.self_attention.attention_out.weight     (768, 768)          0.2820    -0.2075   0.0324    0.0000    nan       nan      
 ctx_model.encoder.layers.5.self_att.self_attention.attention_out.bias       (768,)              0.5625    -0.2228   0.0493    0.0002    nan       nan      
 ctx_model.encoder.layers.5.ffn.layernorm_before_ffn.weight                  (768,)              2.5430    0.6982    0.1042    0.8540    nan       nan      
 ctx_model.encoder.layers.5.ffn.layernorm_before_ffn.bias                    (768,)              0.4653    -1.0654   0.0833    -0.0141   nan       nan      
 ctx_model.encoder.layers.5.ffn.ffn.w_in.w.weight                            (3072, 768)         0.5034    -0.5200   0.0382    -0.0002   nan       nan      
 ctx_model.encoder.layers.5.ffn.ffn.w_in.w.bias                              (3072,)             0.3655    -0.3733   0.0569    -0.0474   nan       nan      
 ctx_model.encoder.layers.5.ffn.ffn.w_out.weight                             (768, 3072)         6.2539    -1.2070   0.0358    -0.0000   nan       nan      
 ctx_model.encoder.layers.5.ffn.ffn.w_out.bias                               (768,)              0.2413    -0.4766   0.0775    -0.0002   nan       nan      
 ctx_model.encoder.layers.6.self_att.layernorm_before_attention.weight       (768,)              1.1465    0.4768    0.0435    0.9946    nan       nan      
 ctx_model.encoder.layers.6.self_att.layernorm_before_attention.bias         (768,)              0.0961    -0.9268   0.0500    -0.0055   nan       nan      
 ctx_model.encoder.layers.6.self_att.self_attention.project_q.weight         (768, 768)          0.4255    -0.3201   0.0486    0.0000    nan       nan      
 ctx_model.encoder.layers.6.self_att.self_attention.project_q.bias           (768,)              0.4880    -0.4524   0.1176    0.0070    nan       nan      
 ctx_model.encoder.layers.6.self_att.self_attention.project_k.weight         (768, 768)          0.3130    -0.3352   0.0466    -0.0000   nan       nan      
 ctx_model.encoder.layers.6.self_att.self_attention.project_k.bias           (768,)              0.0294    -0.0199   0.0057    0.0004    nan       nan      
 ctx_model.encoder.layers.6.self_att.self_attention.project_v.weight         (768, 768)          0.2091    -0.2068   0.0355    0.0001    nan       nan      
 ctx_model.encoder.layers.6.self_att.self_attention.project_v.bias           (768,)              0.0846    -0.1664   0.0257    -0.0005   nan       nan      
 ctx_model.encoder.layers.6.self_att.self_attention.attention_out.weight     (768, 768)          0.2349    -0.2681   0.0325    0.0000    nan       nan      
 ctx_model.encoder.layers.6.self_att.self_attention.attention_out.bias       (768,)              0.8989    -0.2722   0.0607    0.0005    nan       nan      
 ctx_model.encoder.layers.6.ffn.layernorm_before_ffn.weight                  (768,)              2.5527    0.6948    0.0978    0.8638    nan       nan      
 ctx_model.encoder.layers.6.ffn.layernorm_before_ffn.bias                    (768,)              0.6436    -1.8086   0.0945    -0.0159   nan       nan      
 ctx_model.encoder.layers.6.ffn.ffn.w_in.w.weight                            (3072, 768)         0.7007    -0.7964   0.0384    -0.0002   nan       nan      
 ctx_model.encoder.layers.6.ffn.ffn.w_in.w.bias                              (3072,)             0.3269    -0.4194   0.0608    -0.0490   nan       nan      
 ctx_model.encoder.layers.6.ffn.ffn.w_out.weight                             (768, 3072)         7.2539    -0.7451   0.0363    -0.0000   nan       nan      
 ctx_model.encoder.layers.6.ffn.ffn.w_out.bias                               (768,)              0.3132    -0.6548   0.0774    -0.0001   nan       nan      
 ctx_model.encoder.layers.7.self_att.layernorm_before_attention.weight       (768,)              1.0752    0.4927    0.0353    0.9639    nan       nan      
 ctx_model.encoder.layers.7.self_att.layernorm_before_attention.bias         (768,)              0.1799    -1.0996   0.0527    -0.0028   nan       nan      
 ctx_model.encoder.layers.7.self_att.self_attention.project_q.weight         (768, 768)          0.2644    -0.2632   0.0462    -0.0001   nan       nan      
 ctx_model.encoder.layers.7.self_att.self_attention.project_q.bias           (768,)              0.5835    -0.5942   0.1384    -0.0018   nan       nan      
 ctx_model.encoder.layers.7.self_att.self_attention.project_k.weight         (768, 768)          0.2976    -0.2622   0.0449    0.0001    nan       nan      
 ctx_model.encoder.layers.7.self_att.self_attention.project_k.bias           (768,)              0.0258    -0.0220   0.0059    0.0004    nan       nan      
 ctx_model.encoder.layers.7.self_att.self_attention.project_v.weight         (768, 768)          0.1820    -0.1847   0.0364    -0.0000   nan       nan      
 ctx_model.encoder.layers.7.self_att.self_attention.project_v.bias           (768,)              0.1813    -0.1494   0.0339    -0.0014   nan       nan      
 ctx_model.encoder.layers.7.self_att.self_attention.attention_out.weight     (768, 768)          0.2303    -0.2485   0.0337    0.0000    nan       nan      
 ctx_model.encoder.layers.7.self_att.self_attention.attention_out.bias       (768,)              0.8164    -0.4888   0.0619    0.0005    nan       nan      
 ctx_model.encoder.layers.7.ffn.layernorm_before_ffn.weight                  (768,)              2.4375    0.7051    0.0845    0.8335    nan       nan      
 ctx_model.encoder.layers.7.ffn.layernorm_before_ffn.bias                    (768,)              0.7212    -1.8779   0.0909    -0.0147   nan       nan      
 ctx_model.encoder.layers.7.ffn.ffn.w_in.w.weight                            (3072, 768)         0.2917    -0.3008   0.0370    -0.0002   nan       nan      
 ctx_model.encoder.layers.7.ffn.ffn.w_in.w.bias                              (3072,)             0.2864    -0.3928   0.0560    -0.0470   nan       nan      
 ctx_model.encoder.layers.7.ffn.ffn.w_out.weight                             (768, 3072)         6.0234    -0.5698   0.0354    -0.0000   nan       nan      
 ctx_model.encoder.layers.7.ffn.ffn.w_out.bias                               (768,)              0.4167    -0.6602   0.0853    0.0002    nan       nan      
 ctx_model.encoder.layers.8.self_att.layernorm_before_attention.weight       (768,)              1.2637    0.5249    0.0299    0.9590    nan       nan      
 ctx_model.encoder.layers.8.self_att.layernorm_before_attention.bias         (768,)              0.4993    -0.8975   0.0443    -0.0010   nan       nan      
 ctx_model.encoder.layers.8.self_att.self_attention.project_q.weight         (768, 768)          0.2544    -0.2576   0.0449    0.0000    nan       nan      
 ctx_model.encoder.layers.8.self_att.self_attention.project_q.bias           (768,)              0.5479    -0.6108   0.1389    0.0007    nan       nan      
 ctx_model.encoder.layers.8.self_att.self_attention.project_k.weight         (768, 768)          0.3157    -0.2759   0.0439    -0.0000   nan       nan      
 ctx_model.encoder.layers.8.self_att.self_attention.project_k.bias           (768,)              0.0184    -0.0219   0.0057    -0.0001   nan       nan      
 ctx_model.encoder.layers.8.self_att.self_attention.project_v.weight         (768, 768)          0.1986    -0.1844   0.0376    0.0000    nan       nan      
 ctx_model.encoder.layers.8.self_att.self_attention.project_v.bias           (768,)              0.2091    -0.1760   0.0326    0.0010    nan       nan      
 ctx_model.encoder.layers.8.self_att.self_attention.attention_out.weight     (768, 768)          0.1908    -0.2037   0.0347    -0.0000   nan       nan      
 ctx_model.encoder.layers.8.self_att.self_attention.attention_out.bias       (768,)              0.4666    -0.6069   0.0610    0.0001    nan       nan      
 ctx_model.encoder.layers.8.ffn.layernorm_before_ffn.weight                  (768,)              1.8838    0.6914    0.0597    0.8096    nan       nan      
 ctx_model.encoder.layers.8.ffn.layernorm_before_ffn.bias                    (768,)              0.5806    -1.3232   0.0762    -0.0139   nan       nan      
 ctx_model.encoder.layers.8.ffn.ffn.w_in.w.weight                            (3072, 768)         0.4382    -0.2910   0.0368    -0.0001   nan       nan      
 ctx_model.encoder.layers.8.ffn.ffn.w_in.w.bias                              (3072,)             0.3972    -0.3625   0.0493    -0.0481   nan       nan      
 ctx_model.encoder.layers.8.ffn.ffn.w_out.weight                             (768, 3072)         5.2500    -1.0166   0.0361    -0.0000   nan       nan      
 ctx_model.encoder.layers.8.ffn.ffn.w_out.bias                               (768,)              0.3899    -0.5708   0.0799    -0.0000   nan       nan      
 ctx_model.encoder.layers.9.self_att.layernorm_before_attention.weight       (768,)              1.7256    0.6650    0.0357    0.9644    nan       nan      
 ctx_model.encoder.layers.9.self_att.layernorm_before_attention.bias         (768,)              0.7881    -0.5981   0.0439    0.0015    nan       nan      
 ctx_model.encoder.layers.9.self_att.self_attention.project_q.weight         (768, 768)          0.2224    -0.4380   0.0443    -0.0000   nan       nan      
 ctx_model.encoder.layers.9.self_att.self_attention.project_q.bias           (768,)              0.3979    -0.4722   0.1403    -0.0082   nan       nan      
 ctx_model.encoder.layers.9.self_att.self_attention.project_k.weight         (768, 768)          0.3433    -0.2673   0.0436    0.0000    nan       nan      
 ctx_model.encoder.layers.9.self_att.self_attention.project_k.bias           (768,)              0.0266    -0.0243   0.0061    0.0004    nan       nan      
 ctx_model.encoder.layers.9.self_att.self_attention.project_v.weight         (768, 768)          0.2014    -0.1780   0.0357    0.0000    nan       nan      
 ctx_model.encoder.layers.9.self_att.self_attention.project_v.bias           (768,)              0.1313    -0.1469   0.0265    0.0000    nan       nan      
 ctx_model.encoder.layers.9.self_att.self_attention.attention_out.weight     (768, 768)          0.3108    -0.2996   0.0322    0.0000    nan       nan      
 ctx_model.encoder.layers.9.self_att.self_attention.attention_out.bias       (768,)              0.5044    -0.5752   0.0628    0.0003    nan       nan      
 ctx_model.encoder.layers.9.ffn.layernorm_before_ffn.weight                  (768,)              1.7803    0.6450    0.0514    0.8140    nan       nan      
 ctx_model.encoder.layers.9.ffn.layernorm_before_ffn.bias                    (768,)              0.5898    -0.7627   0.0620    -0.0025   nan       nan      
 ctx_model.encoder.layers.9.ffn.ffn.w_in.w.weight                            (3072, 768)         0.3401    -0.2959   0.0370    -0.0002   nan       nan      
 ctx_model.encoder.layers.9.ffn.ffn.w_in.w.bias                              (3072,)             0.2067    -0.3523   0.0417    -0.0483   nan       nan      
 ctx_model.encoder.layers.9.ffn.ffn.w_out.weight                             (768, 3072)         4.4805    -0.7744   0.0369    -0.0000   nan       nan      
 ctx_model.encoder.layers.9.ffn.ffn.w_out.bias                               (768,)              0.5112    -0.4136   0.0726    -0.0001   nan       nan      
 ctx_model.encoder.layers.10.self_att.layernorm_before_attention.weight      (768,)              1.9053    0.6577    0.0408    0.9648    nan       nan      
 ctx_model.encoder.layers.10.self_att.layernorm_before_attention.bias        (768,)              0.9111    -0.3618   0.0492    0.0114    nan       nan      
 ctx_model.encoder.layers.10.self_att.self_attention.project_q.weight        (768, 768)          0.2146    -0.2367   0.0450    0.0001    nan       nan      
 ctx_model.encoder.layers.10.self_att.self_attention.project_q.bias          (768,)              0.7012    -0.7715   0.1681    0.0074    nan       nan      
 ctx_model.encoder.layers.10.self_att.self_attention.project_k.weight        (768, 768)          0.2593    -0.2734   0.0443    -0.0000   nan       nan      
 ctx_model.encoder.layers.10.self_att.self_attention.project_k.bias          (768,)              0.0275    -0.0334   0.0066    0.0000    nan       nan      
 ctx_model.encoder.layers.10.self_att.self_attention.project_v.weight        (768, 768)          0.1831    -0.1777   0.0353    0.0000    nan       nan      
 ctx_model.encoder.layers.10.self_att.self_attention.project_v.bias          (768,)              0.2202    -0.1782   0.0359    -0.0003   nan       nan      
 ctx_model.encoder.layers.10.self_att.self_attention.attention_out.weight    (768, 768)          0.3616    -0.3479   0.0319    -0.0000   nan       nan      
 ctx_model.encoder.layers.10.self_att.self_attention.attention_out.bias      (768,)              0.3875    -0.6060   0.0677    0.0002    nan       nan      
 ctx_model.encoder.layers.10.ffn.layernorm_before_ffn.weight                 (768,)              2.8047    0.6372    0.0759    0.8706    nan       nan      
 ctx_model.encoder.layers.10.ffn.layernorm_before_ffn.bias                   (768,)              0.6445    -0.6567   0.0656    0.0069    nan       nan      
 ctx_model.encoder.layers.10.ffn.ffn.w_in.w.weight                           (3072, 768)         0.3535    -0.2717   0.0380    -0.0001   nan       nan      
 ctx_model.encoder.layers.10.ffn.ffn.w_in.w.bias                             (3072,)             0.3538    -0.2477   0.0302    -0.0476   nan       nan      
 ctx_model.encoder.layers.10.ffn.ffn.w_out.weight                            (768, 3072)         11.5781   -1.8086   0.0375    0.0000    nan       nan      
 ctx_model.encoder.layers.10.ffn.ffn.w_out.bias                              (768,)              0.5063    -0.2345   0.0596    0.0001    nan       nan      
 ctx_model.encoder.layers.11.self_att.layernorm_before_attention.weight      (768,)              1.6553    0.5049    0.0347    0.9663    nan       nan      
 ctx_model.encoder.layers.11.self_att.layernorm_before_attention.bias        (768,)              0.8457    -1.5586   0.0765    0.0176    nan       nan      
 ctx_model.encoder.layers.11.self_att.self_attention.project_q.weight        (768, 768)          0.2126    -0.2150   0.0448    -0.0001   nan       nan      
 ctx_model.encoder.layers.11.self_att.self_attention.project_q.bias          (768,)              0.5225    -0.5620   0.1833    -0.0138   nan       nan      
 ctx_model.encoder.layers.11.self_att.self_attention.project_k.weight        (768, 768)          0.2703    -0.2515   0.0436    0.0001    nan       nan      
 ctx_model.encoder.layers.11.self_att.self_attention.project_k.bias          (768,)              0.0203    -0.0233   0.0062    -0.0000   nan       nan      
 ctx_model.encoder.layers.11.self_att.self_attention.project_v.weight        (768, 768)          0.2479    -0.2107   0.0403    -0.0001   nan       nan      
 ctx_model.encoder.layers.11.self_att.self_attention.project_v.bias          (768,)              0.1011    -0.0900   0.0235    -0.0001   nan       nan      
 ctx_model.encoder.layers.11.self_att.self_attention.attention_out.weight    (768, 768)          1.1289    -1.1084   0.0375    0.0000    nan       nan      
 ctx_model.encoder.layers.11.self_att.self_attention.attention_out.bias      (768,)              0.2664    -0.4788   0.0490    -0.0000   nan       nan      
 ctx_model.encoder.layers.11.ffn.layernorm_before_ffn.weight                 (768,)              1.0146    0.1737    0.0338    0.8687    nan       nan      
 ctx_model.encoder.layers.11.ffn.layernorm_before_ffn.bias                   (768,)              1.1953    -0.1689   0.0853    0.0134    nan       nan      
 ctx_model.encoder.layers.11.ffn.ffn.w_in.w.weight                           (3072, 768)         0.2666    -0.2373   0.0378    0.0002    nan       nan      
 ctx_model.encoder.layers.11.ffn.ffn.w_in.w.bias                             (3072,)             0.8213    -0.4004   0.0554    -0.0498   nan       nan      
 ctx_model.encoder.layers.11.ffn.ffn.w_out.weight                            (768, 3072)         2.1152    -0.9043   0.0357    -0.0000   nan       nan      
 ctx_model.encoder.layers.11.ffn.ffn.w_out.bias                              (768,)              0.3083    -0.1350   0.0458    0.0003    nan       nan      
 ctx_model.encoder.output_layernorm.weight                                   (768,)              1.1426    0.5776    0.0552    0.8110    nan       nan      
 ctx_model.encoder.output_layernorm.bias                                     (768,)              1.6982    -0.5469   0.1503    0.0000    nan       nan      
 ctx_model.lm_head.dense.weight                                              (768, 768)          1.1729    -1.1133   0.0511    -0.0011   0.0000    0.0000   
 ctx_model.lm_head.dense.bias                                                (768,)              4.2969    -0.7007   0.2524    0.0617    0.0000    0.0000   
 ctx_model.lm_head.layer_norm.weight                                         (768,)              3.4258    0.1548    0.3772    2.5840    0.0000    0.0000   
 ctx_model.lm_head.layer_norm.bias                                           (768,)              1.6289    -2.0879   0.3225    0.1052    0.0000    0.0000   
 ctx_model.lm_head.decoder.weight                                            (21128, 768)        0.6973    -1.0859   0.0543    -0.0016   0.0000    0.0000   
 ctx_model.lm_head.decoder.bias                                              (21128,)            2.3516    -5.0078   0.1404    -0.2234   0.0000    0.0000   
 ctx_model.pooler.dense.weight                                               (768, 768)          0.2971    -0.2421   0.0421    0.0000    0.0000    0.0000   
 ctx_model.pooler.dense.bias                                                 (768,)              0.0852    -0.0817   0.0423    0.0026    0.0000    0.0000   
Gradient overflow, change scale from 1048576.000000 to 524288.000000
1         train   1/338        0:12/70:27         56.961  {"acc": 0.0508}	1e-05	grad_norm: nan
Gradient overflow, change scale from 524288.000000 to 262144.000000
Gradient overflow, change scale from 262144.000000 to 131072.000000
Gradient overflow, change scale from 131072.000000 to 65536.000000
Gradient overflow, change scale from 65536.000000 to 32768.000000
Gradient overflow, change scale from 32768.000000 to 16384.000000
Gradient overflow, change scale from 16384.000000 to 8192.000000
Gradient overflow, change scale from 8192.000000 to 4096.000000
1         train   21/338       1:06/16:44         28.490  {"acc": 0.271}	1e-05	grad_norm: 6.36670446395874
Gradient overflow, change scale from 4096.000000 to 2048.000000
1         train   41/338       2:01/14:42         15.432  {"acc": 0.4788}	1e-05	grad_norm: 1.816748023033142
1         train   61/338       2:57/13:27         10.696  {"acc": 0.5764}	1e-05	grad_norm: 1.2938406467437744
1         train   81/338       3:54/12:23         8.259   {"acc": 0.6322}	1e-05	grad_norm: 1.239333987236023
1         train   101/338      4:50/11:21         6.780   {"acc": 0.6673}	1e-05	grad_norm: 1.092138409614563
