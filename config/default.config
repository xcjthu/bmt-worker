[train] #train parameters
epoch = 16
batch_size = 128

shuffle = True

reader_num = 8

optimizer = adam
learning_rate = 1e-3
scheduler = constant
warmup_steps = 0
training_steps = 0

weight_decay = 0
max_grad_norm = 1
fp16=False

valid_mode=batch
save_step = 2000

grad_accumulate = 1

no_valid=False

inspector_para = *




[eval] #eval parameters
batch_size = 128

shuffle = False

reader_num = 4

[distributed]
use = False
backend = nccl

[model]
pretrained_model_path = /data/disk1/private/xcj/PLMs/model-center

[output] #output parameters
output_time = 1
test_time = 1

model_path = ../checkpoint

output_grad = True
output_function = binary
